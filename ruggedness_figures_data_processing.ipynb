{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Limit memory usage to 50% (adjust as needed)\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.7\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "from direvo_functions import *\n",
    "from ruggedness_functions import *\n",
    "import selection_function_library as slct\n",
    "import tqdm\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SLIDE_data path\n",
    "slide_data_dir = \"/home/jess/Documents/SLIDE_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DE function\n",
    "\n",
    "def directedEvolution(rng,\n",
    "                      selection_strategy, \n",
    "                      selection_params, \n",
    "                      empirical = False, \n",
    "                      N = None, \n",
    "                      K = None, \n",
    "                      landscape = None, \n",
    "                      popsize=100, \n",
    "                      mut_chance=0.01, \n",
    "                      num_steps=50, \n",
    "                      num_reps=10, \n",
    "                      define_i_pop=None, \n",
    "                      pre_optimisation_steps=0,\n",
    "                      average=True):\n",
    "    \n",
    " \n",
    "    r1,r2,r3=jr.split(rng,3)\n",
    "\n",
    "    # Get initial population.\n",
    "    if define_i_pop == None:\n",
    "        i_pop = jnp.array([jr.randint(r1, (N,), 0, 2)]*popsize)\n",
    "    else:\n",
    "        i_pop = define_i_pop\n",
    " \n",
    "    # Function for evaluating fitness.\n",
    "    if empirical:\n",
    "        fitness_function = build_empirical_landscape_function(landscape)\n",
    "        mutation_function = build_mutation_function(mut_chance, 20)\n",
    "    else:\n",
    "        fitness_function = build_NK_landscape_function(r2, N, K)\n",
    "        mutation_function = build_mutation_function(mut_chance, 2)\n",
    " \n",
    "    # Define selection function.\n",
    "    selection_function = build_selection_function(\n",
    "        selection_strategy, selection_params)\n",
    "    \n",
    "    if pre_optimisation_steps!= 0:\n",
    "\n",
    "        pre_op_selection_function=build_selection_function(slct.base_chance_threshold_select, {'base_chance':0.0, 'threshold':0.95})\n",
    "\n",
    "\n",
    "        pre_op = run_directed_evolution(r3, i_pop=i_pop, \n",
    "                               selection_function=pre_op_selection_function, \n",
    "                               mutation_function=mutation_function, \n",
    "                               fitness_function=fitness_function, \n",
    "                               num_steps=pre_optimisation_steps)[1]\n",
    " \n",
    "        i_pop = pre_op['pop'][-1]\n",
    "\n",
    "    # Bringing it all together.\n",
    "    vmapped_run = jax.jit(jax.vmap(lambda r: run_directed_evolution(\n",
    "        r, i_pop, selection_function, mutation_function, fitness_function=fitness_function, num_steps=num_steps)[1]))\n",
    "    \n",
    "    # The array of seeds we will take as input.\n",
    "    rng_seeds = jr.split(r3, num_reps)\n",
    "    results = vmapped_run(rng_seeds)\n",
    " \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NK Ruggedness Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NK_grid(N_range, num_samples=10):\n",
    "    N = jnp.linspace(N_range[0], N_range[1], num=num_samples)\n",
    "    K = jnp.array([jnp.linspace(1, i, num_samples)\n",
    "                  for i in N]).reshape(num_samples, num_samples)\n",
    "    N = jnp.repeat(N, num_samples).reshape(num_samples, num_samples)\n",
    "    return N, K\n",
    "\n",
    "N_grid, K_grid = NK_grid([10, 50])\n",
    "\n",
    "Ns, Ks = N_grid.flatten(), K_grid.flatten()\n",
    "Ns = jnp.flip(Ns)\n",
    "Ks = jnp.flip(Ks)\n",
    "NKs = list(zip(Ns,Ks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Large sweep data\n",
    "\n",
    "file_path = os.path.join(slide_data_dir, \"large_strategy_sweep.pkl\")\n",
    "with open(file_path, \"rb\") as f:\n",
    "    strategy_data = pickle.load(f)\n",
    "\n",
    "file_path = os.path.join(slide_data_dir, \"large_decay_curve_sweep.pkl\")\n",
    "with open(file_path, \"rb\") as f:\n",
    "    decay_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example fitness decay curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "NK_pair = [(20,14), (20,1)]\n",
    "#NK_pair = [(40,15), (40,2)]\n",
    "out = []\n",
    "params = {'threshold': 0.0, 'base_chance' : 1.0}\n",
    "for nk in NK_pair:\n",
    "    run = directedEvolution(jr.PRNGKey(0),\n",
    "                            N = int(nk[0]),\n",
    "                            K=int(nk[1]),\n",
    "                            selection_strategy=slct.base_chance_threshold_select,\n",
    "                            selection_params = params,\n",
    "                            popsize=int(300),#int(150),\n",
    "                            mut_chance=0.5/int(nk[0]),#0.1/int(nk[0]),\n",
    "                            num_steps=25,\n",
    "                            num_reps=1,\n",
    "                            pre_optimisation_steps=20,\n",
    "                            average=True)\n",
    "    out.append(run['fitness'].mean(axis=-1))\n",
    "\n",
    "smooth_rugged = np.array([out[i][0]/out[i][0][0] for i in range(2)]) # Normalising to decay from 1\n",
    "smooth_rugged_decay_rates = np.array([get_single_decay_rate(i, mut = 0.5) for i in smooth_rugged])\n",
    "mutations = np.linspace(0.0,12,25)\n",
    "fitted_lines = np.array([( np.exp(-mutations*(i[0]))*(1 - i[-1]) + i[-1]) for i in smooth_rugged_decay_rates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/smooth_rugged_example.pkl', 'wb') as f:\n",
    "    pickle.dump((smooth_rugged, fitted_lines), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ruggedness prediction accuracy over $K/N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_decay_curves = decay_data.reshape(100,-1,25)\n",
    "normalized_decay_curves = reshaped_decay_curves / reshaped_decay_curves[:, :, 0][:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rates = np.zeros((normalized_decay_curves.shape[0], normalized_decay_curves.shape[1]))\n",
    "\n",
    "for i in range(normalized_decay_curves.shape[0]):\n",
    "    for ii in range(normalized_decay_curves.shape[1]):\n",
    "        decay_rates[i,ii] = get_single_decay_rate(normalized_decay_curves[i,ii,:], mut=0.5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_plus_one_over_ns = np.clip((np.array(NKs)[:,1] + 1)/np.array(NKs)[:,0], 0, 1)\n",
    "k_over_ns = np.clip((np.array(NKs)[:,1])/np.array(NKs)[:,0], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/ruggedness_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump((k_plus_one_over_ns, decay_rates), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring accuracy over popsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(slide_data_dir, \"popsize_accuracy.pkl\")\n",
    "with open(file_path, \"rb\") as f:\n",
    "    popsize_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_popsize_curves = popsize_data.reshape(25,-1,25)\n",
    "popsize_curves = reshaped_popsize_curves / reshaped_popsize_curves[:,:, 0][:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "popsize_decay_rates = np.zeros((25,500))\n",
    "for i in range(25):\n",
    "    for ii in range(500):\n",
    "        popsize_decay_rates[i,ii] = get_single_decay_rate(popsize_curves[i,ii,:], mut=1.0)[0]\n",
    "\n",
    "pops=np.linspace(100,2500,25, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/popsize_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump((popsize_decay_rates, pops), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring accuracy over mutation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(slide_data_dir, \"mutation_rate_accuracy.pkl\")\n",
    "with open(file_path, \"rb\") as f:\n",
    "    mut_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_mut_curves = mut_data.reshape(25,-1,25)\n",
    "mut_curves = reshaped_mut_curves / reshaped_mut_curves[:,:, 0][:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "muts=np.linspace(0.01,2,25)\n",
    "\n",
    "mut_decay_rates = np.zeros((25,500))\n",
    "for i in range(25):\n",
    "    for ii in range(500):\n",
    "        mut_decay_rates[i,ii] = get_single_decay_rate(mut_curves[i,ii,:], mut=muts[i])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/mut_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump((mut_decay_rates, muts), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of ruggedness metrics on NK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:35<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:35<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "test_NKs = np.array([[12,0],[12,1],[12,2],[12,3],[12,4],[12,5],[12,6],[12,7],[12,8],[12,9],[12,10],[12,11]])\n",
    "shape = (2,) * int(test_NKs[0][0])\n",
    "NK_roughness_to_slope = []\n",
    "NK_fourier = []\n",
    "convergence_rates = []\n",
    "convergence_rates_extra = []\n",
    "NK_paths_to_max = []\n",
    "NK_closest_max = []\n",
    "NK_local_epistasis = []\n",
    "rng = jr.PRNGKey(42)\n",
    "rng_list = jr.split(rng, 10)\n",
    "\n",
    "for N, K in test_NKs:\n",
    "    print(N, K)\n",
    "    for r in tqdm.tqdm(rng_list):\n",
    "        N = int(N)\n",
    "        K = int(K)\n",
    "        complete_landscape = get_nk_l_o_shape(r, N, K,shape)\n",
    "        NK_roughness_to_slope.append(roughness_to_slope(complete_landscape))\n",
    "        NK_fourier.append(landscape_r2(complete_landscape))\n",
    "        NK_paths_to_max.append(get_mean_paths_to_max(complete_landscape, norm=False, extra_slack = 0))\n",
    "        NK_closest_max.append(1/find_distance_to_closest_max(complete_landscape))\n",
    "        NK_local_epistasis.append(local_epistasis(complete_landscape,[0,]*12)), \n",
    "        cr, extr = get_convergence_rate(r, N, K)\n",
    "        convergence_rates.append(cr)\n",
    "        convergence_rates_extra.append(extr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "NK_roughness_to_slope_r = np.array(NK_roughness_to_slope).reshape(12,10)\n",
    "NK_fourier_r = np.array(NK_fourier).reshape(12,10)\n",
    "convergence_rates_r = np.array(convergence_rates).reshape(12,10)\n",
    "NK_paths_to_max_r = np.array(NK_paths_to_max).reshape(12,10)\n",
    "NK_closest_max_r = np.array(NK_closest_max).reshape(12,10)\n",
    "NK_local_epistasis_r = np.array([i['simple_sign_episasis'] + i['reciprocal_sign_epistasis'] for i in NK_local_epistasis]).reshape(12,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_NK_roughness_to_slope = [np.array(i).mean() for i in NK_roughness_to_slope_r]\n",
    "mean_NK_fourier = [np.array(i).mean() for i in NK_fourier_r]\n",
    "mean_convergence_rates = [np.array(i).mean() for i in  convergence_rates_r]\n",
    "#mean_convergence_rates_extra = [np.array(i).mean() for i in convergence_rates_extra]\n",
    "mean_NK_paths_to_max = [np.array(i).mean() for i in NK_paths_to_max_r]\n",
    "mean_NK_closest_max = [np.array(i).mean() for i in NK_closest_max_r]\n",
    "mean_NK_local_epistasis = [np.array(i).mean() for i in NK_local_epistasis_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_NK_closest_max = 1 - 1/np.array(mean_NK_closest_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(data):\n",
    "    return np.array(( data- min(data) )/( max(data)-min(data) ))\n",
    "\n",
    "mean_NK_roughness_to_slope = norm_data(np.array(mean_NK_roughness_to_slope))\n",
    "mean_NK_fourier = norm_data(np.array(mean_NK_fourier))\n",
    "mean_NK_convergence_rates = norm_data(np.array(mean_convergence_rates))\n",
    "mean_NK_paths_to_max = norm_data(np.array(mean_NK_paths_to_max))\n",
    "mean_NK_closest_max = norm_data(np.array(mean_NK_closest_max))\n",
    "mean_NK_le_normed = norm_data(np.array(mean_NK_local_epistasis))\n",
    "\n",
    "\n",
    "k_over_ns = (test_NKs[:,1]+1)/test_NKs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/NK_ruggedness_metric_comparison.pkl', 'wb') as f:\n",
    "    pickle.dump((mean_NK_roughness_to_slope, mean_NK_fourier, mean_NK_convergence_rates, mean_NK_paths_to_max, mean_NK_closest_max, k_over_ns, mean_NK_le_normed), f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical ruggedness prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of ruggedness metrics on real landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('landscape_arrays/GB1_landscape_array.pkl', 'rb') as f:\n",
    "    GB1 = pickle.load(f)\n",
    "\n",
    "with open('landscape_arrays/E3_landscape_array.pkl', 'rb') as f:\n",
    "    ParD3 = pickle.load(f)\n",
    "\n",
    "with open('landscape_arrays/TEV_landscape_array.pkl', 'rb') as f:\n",
    "    TEV = pickle.load(f)\n",
    "\n",
    "with open('landscape_arrays/TrpB_landscape_array.pkl', 'rb') as f:\n",
    "    TrpB = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_landscapes = [GB1, TrpB, TEV, ParD3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decay rate\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_gb1_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    gb1_decay = pickle.load(f)\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_trpb_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    trpb_decay = pickle.load(f)\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_tev_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    tev_decay = pickle.load(f)\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_pard3_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    pard3_decay = pickle.load(f)\n",
    "\n",
    "decay_rate_measurements = [(i**2).mean(axis=(0,1,2)) for i in [gb1_decay, trpb_decay, tev_decay, pard3_decay]]\n",
    "decay_rate_measurements = [i/i[0] for i in decay_rate_measurements]\n",
    "decay_rate_measurements = [get_single_decay_rate(i)[0]/2 for i in decay_rate_measurements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roughness to slope\n",
    "roughness_to_slope_measurements  = [roughness_to_slope(i) for i in empirical_landscapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landscape R2\n",
    "landscape_r2_measurements = [1-landscape_r2(i) for i in empirical_landscapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local epistasis\n",
    "starting_points = [[3,17,0,3],[3,8,3,18],[19,17,11,18],[17,12,16]]\n",
    "landscapes_and_starts = list(zip(empirical_landscapes,starting_points))\n",
    "local_epistasis_measurements = [local_epistasis(i, np.array(ii)) for i, ii in landscapes_and_starts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to global maximum\n",
    "paths_to_max_measurements = [get_mean_paths_to_max(i, norm=False) for i in empirical_landscapes]\n",
    "paths_to_max_measurements[3] = paths_to_max_measurements[3]*(max_possible_paths(GB1.shape)/max_possible_paths(ParD3.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to closest local maximum\n",
    "local_max_measurements = [find_distance_to_closest_max(i) for i in empirical_landscapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/empirical_ruggedness_metric_comparison.pkl', 'wb') as f:\n",
    "    pickle.dump((decay_rate_measurements, roughness_to_slope_measurements, landscape_r2_measurements, local_epistasis_measurements, paths_to_max_measurements, local_max_measurements), f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier spectra of empirical landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_spectrum_gb1 = get_landscape_spectrum(GB1, remove_constant= False, on_gpu=True, norm = True)\n",
    "full_spectrum_trpb = get_landscape_spectrum(TrpB, remove_constant= False, on_gpu=True, norm = True)\n",
    "full_spectrum_tev = get_landscape_spectrum(TEV, remove_constant= False, on_gpu=True, norm = True)\n",
    "full_spectrum_pard3 = get_landscape_spectrum(ParD3, remove_constant= False, on_gpu=True, norm = True)\n",
    "\n",
    "with open('plot_data/fourier_spectra_empirical.pkl', 'wb') as f:\n",
    "    pickle.dump((full_spectrum_gb1 , full_spectrum_trpb, full_spectrum_tev, full_spectrum_pard3),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = [d.reshape(-1, 25) for d in [gb1_decay, trpb_decay, tev_decay, pard3_decay]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sq_subsample(rng, flat_data, num_samples=100, num_reps = 100):\n",
    "    \"\"\"\n",
    "    Generate a subsample of the squared data for plotting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def single_sample(rng, flat_data):\n",
    "        \"\"\"\n",
    "        Generate a single sample of the squared data.\n",
    "        \"\"\"\n",
    "        idx = jr.choice(rng, jnp.arange(flat_data.shape[0]), shape=(num_samples,), replace=False)\n",
    "        sq_data = jnp.mean(flat_data[idx]**2, axis = 0)\n",
    "        normed_data = sq_data - sq_data[-1]\n",
    "        normed_data = normed_data / normed_data[0]\n",
    "        return normed_data\n",
    "    \n",
    "\n",
    "    rngs = jr.split(rng, num_reps)\n",
    "    samples = jax.vmap(jax.jit(single_sample), in_axes=(0, None))(rngs, flat_data)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng=jr.PRNGKey(0)\n",
    "\n",
    "bad_subsampy = generate_sq_subsample(rng, flat_data[0], num_samples=10, num_reps=200)\n",
    "subsampy = generate_sq_subsample(rng, flat_data[0], num_samples=100, num_reps=200)\n",
    "good_subsampy = generate_sq_subsample(rng, flat_data[0], num_samples=1000, num_reps=200)\n",
    "\n",
    "bad_curve_params = jnp.array([get_single_decay_rate(s) for s in bad_subsampy])\n",
    "curve_params = jnp.array([get_single_decay_rate(s) for s in subsampy])\n",
    "good_curve_params = jnp.array([get_single_decay_rate(s) for s in good_subsampy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 30s\n",
    "num_reps_used = 200\n",
    "num_samples_totest = np.logspace(1,3, num=8, dtype=int)\n",
    "\n",
    "results_means = []\n",
    "results_stds = []\n",
    "for d in flat_data:\n",
    "    results_l_m = []\n",
    "    results_l_s = []\n",
    "    for num_samples in num_samples_totest:\n",
    "        subsampy = generate_sq_subsample(rng, d, num_samples=num_samples, num_reps=num_reps_used)\n",
    "        curve_params = jnp.array([get_single_decay_rate(s) for s in subsampy])\n",
    "        results_l_m.append(curve_params[:,0].mean())\n",
    "        results_l_s.append(jnp.std(curve_params[:,0], axis=0))\n",
    "\n",
    "    results_means.append(results_l_m)\n",
    "    results_stds.append(results_l_s)\n",
    "\n",
    "results_means = jnp.array(results_means)/2\n",
    "results_stds = jnp.array(results_stds)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['GB1', 'TrpB', 'TEV', 'ParD3']\n",
    "\n",
    "with open('plot_data/subsampling_empirical_estimates.pkl', 'wb') as f:\n",
    "    pickle.dump((num_samples_totest, results_means, results_stds, data_names), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising directed evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal DE strategies from sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking the mean over N.\n",
    "reshaped_strategies = strategy_data.reshape(100, -1, 300)\n",
    "N_meaned_strategies = reshaped_strategies[:90].mean(axis=2).reshape(9,10,49).mean(axis=0)\n",
    "N_meaned_decay_data = normalized_decay_curves.reshape(10,10,250,25).mean(axis=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rates = []\n",
    "\n",
    "for i in N_meaned_decay_data:\n",
    "    decay_rates.append(get_single_decay_rate(i,mut=0.5)[0])\n",
    "    \n",
    "decay_rates = np.array(decay_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting optimal base chances and splits from strategy sweep.\n",
    "\n",
    "thresholds, base_chances = base_chance_threshold_fixed_prop([0,0.19], 0.2, 7)\n",
    "splits = [24,20,16,12,8,4,1]\n",
    "\n",
    "base_chance_array = np.array([base_chances]*7).flatten()\n",
    "splitting_array = np.array([[24]*7,[20]*7,[16]*7,[12]*7,[8]*7,[4]*7,[1]*7]).flatten()\n",
    "\n",
    "optimal_splits = []\n",
    "optimal_base_chances = []\n",
    "\n",
    "for i in range(10):\n",
    "    max_val = N_meaned_strategies[i].argmax()\n",
    "    optimal_splits.append(splitting_array[max_val])\n",
    "    optimal_base_chances.append(base_chance_array[max_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/optimal_DE_strategies.pkl', 'wb') as f:\n",
    "    pickle.dump((decay_rates, optimal_splits, optimal_base_chances), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of strategy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "NKs = np.array(NKs)\n",
    "k_over_ns = (NKs[:,1]+1)/NKs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions from 10 isolated runs\n",
    "\n",
    "predicted_base_chances = []\n",
    "predicted_splittings = []\n",
    "actual_k_over_ns = []\n",
    "\n",
    "for landscape in range(normalized_decay_curves.shape[0]):\n",
    "\n",
    "    landscape_outcomes = []\n",
    "\n",
    "    # For each ld, take mean across all reps and reshape into strategy space.\n",
    "    landscape_strategy_outcomes = reshaped_strategies[landscape,:,:].mean(axis=1).reshape(7,7)\n",
    "\n",
    "    for run in range(100):\n",
    "\n",
    "        ### Estimate decay rate (k/n).\n",
    "        run_data = normalized_decay_curves[landscape, run,:]\n",
    "        run_decay_rate = get_single_decay_rate(run_data,mut=0.5)[0]\n",
    "        actual_k_over_ns.append(k_over_ns[landscape])\n",
    "\n",
    "        ### Collect predicted vs actual \n",
    "        predicted_base_chance = optimal_base_chances[np.argmin(np.abs(decay_rates - run_decay_rate))]\n",
    "        predicted_splitting = optimal_splits[np.argmin(np.abs(decay_rates - run_decay_rate))]\n",
    "        predicted_base_chances.append(predicted_base_chance)\n",
    "        predicted_splittings.append(predicted_splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_actual = np.round(actual_k_over_ns, 1)\n",
    "\n",
    "unique_x = np.unique(rounded_actual)\n",
    "\n",
    "bc_means = []\n",
    "bc_stds = []\n",
    "\n",
    "for x in unique_x:\n",
    "    # Select points corresponding to the current x value\n",
    "    bc_values = np.array(predicted_base_chances)[rounded_actual == x]\n",
    "    bc_means.append(np.mean(bc_values))\n",
    "    bc_stds.append(np.std(bc_values))\n",
    "\n",
    "sp_means = []\n",
    "sp_stds = []\n",
    "\n",
    "for x in unique_x:\n",
    "    # Select points corresponding to the current x value\n",
    "    sp_values = np.array(predicted_splittings)[rounded_actual == x]\n",
    "    sp_means.append(np.mean(sp_values))\n",
    "    sp_stds.append(np.std(sp_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/strategy_prediction_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump((actual_k_over_ns, bc_means, bc_stds, sp_means, sp_stds), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NK directed evo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predicted base chance and splitting values from N = 45, K = [1,25,45]\n",
    "\n",
    "NK_samples = [(45,1), (45,25),(45,1), (45,25),]\n",
    "indexes_of_interest = [1900,1400,1000]\n",
    "NK_bc_predictions = [0.0,0.0]\n",
    "NK_th_predictions = [0.8,0.8]\n",
    "NK_sp_predictions = [1,1]\n",
    "\n",
    "for i in indexes_of_interest:\n",
    "\n",
    "    mean_bc_pred = np.array(predicted_base_chances[i:i+100]).mean()\n",
    "    mean_sp_pred = np.array(predicted_splittings[i:i+100]).mean()\n",
    "\n",
    "    # Get the closest to the standard values.\n",
    "    bc = base_chances[np.argmin(np.abs(base_chances - mean_bc_pred))]\n",
    "    th = thresholds[np.argmin(np.abs(base_chances - mean_bc_pred))]\n",
    "    sp = splits[np.argmin(np.abs(splits - mean_sp_pred))]\n",
    "\n",
    "    NK_bc_predictions.append(bc)\n",
    "    NK_th_predictions.append(th)\n",
    "    NK_sp_predictions.append(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of the DE function is modified to allow splits (by ensuring each split begins from the same starting location).\n",
    "\n",
    "def directedEvolution(s_rng, \n",
    "                      rng_rep,\n",
    "                      selection_strategy, \n",
    "                      selection_params, \n",
    "                      empirical = False, \n",
    "                      N = None, \n",
    "                      K = None, \n",
    "                      landscape = None, \n",
    "                      popsize=100, \n",
    "                      mut_chance=0.01, \n",
    "                      num_steps=50, \n",
    "                      num_reps=10, \n",
    "                      define_i_pop=None, \n",
    "                      average=True):\n",
    "    \n",
    " \n",
    "    # Get initial population.\n",
    "    if define_i_pop == None:\n",
    "        i_pop = jnp.array([jr.randint(rng_rep, (N,), 0, 2)]*popsize)\n",
    "    else:\n",
    "        i_pop = define_i_pop\n",
    " \n",
    "    # Function for evaluating fitness.\n",
    "    if empirical:\n",
    "        fitness_function = build_empirical_landscape_function(landscape)\n",
    "        mutation_function = build_mutation_function(mut_chance, 20)\n",
    "    else:\n",
    "        fitness_function = build_NK_landscape_function(rng_rep, N, K)\n",
    "        mutation_function = build_mutation_function(mut_chance, 2)\n",
    " \n",
    "    # Define selection function.\n",
    "    selection_function = build_selection_function(\n",
    "        selection_strategy, selection_params)\n",
    " \n",
    "    # Bringing it all together.\n",
    "    vmapped_run = jax.jit(jax.vmap(lambda r: run_directed_evolution(\n",
    "        r, i_pop, selection_function, mutation_function, fitness_function=fitness_function, num_steps=num_steps)[1]))\n",
    "    \n",
    "    # The array of seeds we will take as input.\n",
    "    rng_seeds = jr.split(s_rng, num_reps)\n",
    "    results = vmapped_run(rng_seeds)\n",
    " \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:  45 , K:  1 , bc:  0.0 , sp:  1\n",
      "N:  45 , K:  25 , bc:  0.0 , sp:  1\n",
      "N:  45 , K:  1 , bc:  0.0 , sp:  4\n",
      "N:  45 , K:  25 , bc:  0.0 , sp:  24\n"
     ]
    }
   ],
   "source": [
    "three_NK_results = []\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    N,K = NK_samples[i]\n",
    "    bc = NK_bc_predictions[i]\n",
    "    s = NK_sp_predictions[i]\n",
    "    th = NK_th_predictions[i]\n",
    "    p=1200\n",
    "    m=0.1\n",
    "    params = {'base chance':bc, 'threshold': th}\n",
    "\n",
    "    print('N: ', N, ', K: ', K, ', bc: ', bc, ', sp: ', s)\n",
    "\n",
    "    rep_rngs = jr.split(jr.PRNGKey(42),100)\n",
    "\n",
    "    def single_rep(rng_rep):\n",
    "\n",
    "        split_rngs = jr.split(rng_rep, s)\n",
    "\n",
    "        def single_s(s_rng):\n",
    "            params = {'threshold': th, 'base_chance' : bc}\n",
    "            run = directedEvolution(s_rng,\n",
    "                                    rng_rep,\n",
    "                                    N = int(N),\n",
    "                                    K=int(K),\n",
    "                                    selection_strategy=slct.base_chance_threshold_select,\n",
    "                                    selection_params = params,\n",
    "                                    popsize=int(p/s),\n",
    "                                    mut_chance=m/int(N),\n",
    "                                    num_steps=50,\n",
    "                                    num_reps=1,\n",
    "                                    average=True)\n",
    "\n",
    "            #split_results.append(run['fitness'].max(axis=2).mean(axis=0)[-1])\n",
    "            ##return run['fitness'][:,:,-1].max(axis=1).mean()\n",
    "            return run['fitness'][:,:,:].mean(axis=0)\n",
    "\n",
    "        split_results = jax.vmap(single_s)(split_rngs)\n",
    "\n",
    "        return jnp.array(split_results)\n",
    "\n",
    "    repeat_results = jax.vmap(single_rep)(rep_rngs)\n",
    "\n",
    "    three_NK_results.append(repeat_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting winning splits in post\n",
    "new_output = []\n",
    "for i in three_NK_results:\n",
    "    winning_splits_only = []\n",
    "    for ii in range(i.shape[0]): # for each rep\n",
    "        rep = i[ii]\n",
    "        final_max = rep[:,-1,:].max(axis=1)\n",
    "        winning_split = np.argmax(final_max)\n",
    "        winning_splits_only.append(rep[winning_split])\n",
    "    new_output.append(np.array(winning_splits_only).mean(axis=(0,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_data/NK_DE.pkl', 'wb') as f:\n",
    "    pickle.dump(new_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_strategies = strategy_data.reshape(100, -1, 300)\n",
    "with open('plot_data/NK_strategy_spaces.pkl', 'wb') as f:\n",
    "    pickle.dump((reshaped_strategies[19,:,:],reshaped_strategies[14,:,:]), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed evolution on real landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(slide_data_dir, \"N4A20_strategy_sweep.pkl\"), \"rb\") as f:\n",
    "    strategy_data = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"N4A20_decay_curves.pkl\"), \"rb\") as f:\n",
    "    decay_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rates = []\n",
    "decay_means = []\n",
    "for n, i in enumerate(decay_data[1:]):\n",
    "    decay_mean = (i**2).mean(axis=(0,1,2))\n",
    "    decay_mean = decay_mean / decay_mean[0]\n",
    "    decay_means.append(decay_mean)\n",
    "    decay_rate = get_single_decay_rate(decay_mean,mut=0.1)\n",
    "    decay_rates.append(decay_rate[0]/2)\n",
    "\n",
    "optimal_pos = []\n",
    "for n, i in enumerate(np.array(strategy_data).mean(axis=0)):\n",
    "\n",
    "    # Find the index of the maximum value\n",
    "    max_pos = np.unravel_index(np.argmax(i.T), i.shape)\n",
    "    optimal_pos.append(max_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving data\n",
    "with open('plot_data/empirical_lookup.pkl', 'wb') as f:\n",
    "    pickle.dump((decay_rates, decay_means, optimal_pos, np.array(strategy_data).mean(axis=0)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds, base_chances = base_chance_threshold_fixed_prop([0,0.19], 0.2, 7)\n",
    "splits = [24,20,16,12,8,4,1]\n",
    "\n",
    "optimal_base_chances = [base_chances[i[1]] for i in optimal_pos]\n",
    "optimal_splits = [splits[i[0]] for i in optimal_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all files\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_gb1_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    GB1_decay_multi = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_trpb_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    TrpB_decay_multi = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_tev_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    TEV_decay_multi = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"decay_curves_pard3_m0.1_multistart_10000_uniform.pkl\"), \"rb\") as f:\n",
    "    ParD3_decay_multi = pickle.load(f)\n",
    "\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"strategy_sweep_GB1_multistart_100_uniform_m0.025.pkl\"), \"rb\") as f:\n",
    "    GB1_sweep_multi = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"strategy_sweep_TrpB_multistart_100_uniform_m0.025.pkl\"), \"rb\") as f:\n",
    "    TrpB_sweep_multi = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"strategy_sweep_TEV_multistart_100_uniform_m0.025.pkl\"), \"rb\") as f:\n",
    "    TEV_sweep_multi = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"strategy_sweep_E3_multistart_100_uniform_m0.025.pkl\"), \"rb\") as f:\n",
    "    ParD3_sweep_multi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_strategy_selection(decay, sweep, decay_rates, optimal_base_chances, optimal_splits, N=4):\n",
    "\n",
    "    def strategy_from_decay(decay_rate, standard_decay_rates = decay_rates, optimal_base_chances = optimal_base_chances, optimal_splits = optimal_splits):\n",
    "        \n",
    "        optimum_index = np.argmin(np.abs(standard_decay_rates - decay_rate))\n",
    "        \n",
    "        return optimal_base_chances[optimum_index], optimal_splits[optimum_index]\n",
    "\n",
    "    # Compute data needed for all subplots\n",
    "    #decay_mean = decay.mean(axis=(0,1))\n",
    "    decay_mean = (decay**2).mean(axis=(0,1,2))\n",
    "    decay_mean = decay_mean / decay_mean[0]\n",
    "    decay_rate = get_single_decay_rate(decay_mean,mut=0.1)\n",
    "    x_vals = np.linspace(0, 24, 25)\n",
    "    strategy = strategy_from_decay(decay_rate[0]/2)\n",
    "\n",
    "    if N==3:\n",
    "        scipy_freq_matrix = np.zeros((5, 5), dtype=int)\n",
    "        _, base_chances = base_chance_threshold_fixed_prop([0,0.19], 0.2, 5)\n",
    "        splits = [20,15,10,5,1]\n",
    "    else:\n",
    "        scipy_freq_matrix = np.zeros((7, 7), dtype=int)\n",
    "        _, base_chances = base_chance_threshold_fixed_prop([0,0.19], 0.2, 7)\n",
    "        splits = [24,20,16,12,8,4,1]\n",
    "\n",
    "    j = np.where(np.array(base_chances) == strategy[0])[0]\n",
    "    i = np.where(np.array(splits) == strategy[1])[0]\n",
    "    scipy_freq_matrix[i,j] = 1\n",
    "\n",
    "    return(x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, strategy)\n",
    "\n",
    "def uniform_start_locs(ld, num=10000):\n",
    "    flat_ld = ld.flatten()\n",
    "    flat_indexes = np.round(np.linspace(0, flat_ld.shape[0]-1, num)).astype(int)\n",
    "    indexes = np.array([np.unravel_index(i, ld.shape) for i in flat_indexes])\n",
    "    return indexes\n",
    "\n",
    "def test_strategy_empirical(ld, bcs, sps, ths, starts):\n",
    "\n",
    "    ld = jnp.array(ld)\n",
    "\n",
    "    start_results = []\n",
    "\n",
    "    for start in tqdm.tqdm(starts):\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i in range(2):\n",
    "\n",
    "            bc = bcs[i]\n",
    "            s = sps[i]\n",
    "            th = ths[i]\n",
    "            if len(ld.shape) == 4:\n",
    "                p=1200\n",
    "            if len(ld.shape) == 3:\n",
    "                p=60\n",
    "            m=0.01\n",
    "\n",
    "            rep_rngs = jr.split(jr.PRNGKey(42),100)\n",
    "\n",
    "            def single_rep(rng_rep):\n",
    "\n",
    "                split_rngs = jr.split(rng_rep, s)\n",
    "\n",
    "                def single_s(s_rng):\n",
    "                    params = {'threshold': th, 'base_chance' : bc}\n",
    "                    \n",
    "                    run = directedEvolution(s_rng,\n",
    "                                            rng_rep,\n",
    "                                            selection_strategy=slct.base_chance_threshold_select,\n",
    "                                            selection_params = params,\n",
    "                                            popsize=int(p/s),\n",
    "                                            mut_chance=m,\n",
    "                                            num_steps=80,\n",
    "                                            num_reps=1,\n",
    "                                            define_i_pop=jnp.array([start]*int(p/s)),\n",
    "                                            empirical=True,\n",
    "                                            landscape=ld,\n",
    "                                            average=True)\n",
    "\n",
    "                    return run['fitness'][:,:,:].mean(axis=0)\n",
    "\n",
    "                split_results = jax.vmap(single_s)(split_rngs)\n",
    "\n",
    "                return jnp.array(split_results)\n",
    "\n",
    "            repeat_results = jax.vmap(single_rep)(rep_rngs)\n",
    "\n",
    "            results.append(repeat_results)\n",
    "\n",
    "        ## Extracting winning splits in post\n",
    "        run = []\n",
    "        for i in results:\n",
    "            winning_splits_only = []\n",
    "            for ii in range(i.shape[0]): # for each rep\n",
    "                rep = i[ii]\n",
    "                final_max = rep[:,-1,:].max(axis=1)\n",
    "                winning_split = np.argmax(final_max)\n",
    "                winning_splits_only.append(rep[winning_split])\n",
    "            run.append(np.array(winning_splits_only).mean(axis=(0,2)))\n",
    "\n",
    "        start_results.append(run)\n",
    "\n",
    "    return start_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "## GB1\n",
    "\n",
    "x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, strategy = empirical_strategy_selection(GB1_decay_multi, GB1_sweep_multi, decay_rates, optimal_base_chances, optimal_splits)\n",
    "\n",
    "run = test_strategy_empirical(GB1, [0.0,strategy[0]], \n",
    "                        [1,strategy[1]], \n",
    "                        [0.8,thresholds[np.array(base_chances) == strategy[0]]],\n",
    "                        starts=uniform_start_locs(ld=GB1, num=10))\n",
    "\n",
    "## Saving data\n",
    "with open('plot_data/GB1_strategy_selection.pkl', 'wb') as f:\n",
    "    pickle.dump((x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, run), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "## TrpB\n",
    "\n",
    "x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, strategy = empirical_strategy_selection(TrpB_decay_multi, TrpB_sweep_multi, decay_rates, optimal_base_chances, optimal_splits)\n",
    "\n",
    "run = test_strategy_empirical(TrpB, [0.0,strategy[0]], \n",
    "                        [1,strategy[1]], \n",
    "                        [0.8,thresholds[np.array(base_chances) == strategy[0]]],\n",
    "                        starts=uniform_start_locs(ld=TrpB, num=10))\n",
    "\n",
    "## Saving data\n",
    "with open('plot_data/TrpB_strategy_selection.pkl', 'wb') as f:\n",
    "    pickle.dump((x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, run), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "## TEV\n",
    "\n",
    "x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, strategy = empirical_strategy_selection(TEV_decay_multi, TEV_sweep_multi, decay_rates, optimal_base_chances, optimal_splits)\n",
    "\n",
    "run = test_strategy_empirical(TEV, [0.0,strategy[0]], \n",
    "                        [1,strategy[1]], \n",
    "                        [0.8,thresholds[np.array(base_chances) == strategy[0]]],\n",
    "                        starts=uniform_start_locs(ld=TEV, num=10))\n",
    "\n",
    "## Saving data\n",
    "with open('plot_data/TEV_strategy_selection.pkl', 'wb') as f:\n",
    "    pickle.dump((x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, run), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(slide_data_dir, \"N3A20_strategy_sweep.pkl\"), \"rb\") as f:\n",
    "    strategy_data = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(slide_data_dir, \"N3A20_decay_curves.pkl\"), \"rb\") as f:\n",
    "    decay_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rates = []\n",
    "decay_means = []\n",
    "for n, i in enumerate(decay_data[1:]):\n",
    "    decay_mean = (i**2).mean(axis=(0,1,2))\n",
    "    decay_mean = decay_mean / decay_mean[0]\n",
    "    decay_means.append(decay_mean)\n",
    "    decay_rate = get_single_decay_rate(decay_mean,mut=0.1)\n",
    "    decay_rates.append(decay_rate[0]/2)\n",
    "\n",
    "optimal_pos = []\n",
    "for n, i in enumerate(np.array(strategy_data).mean(axis=0)):\n",
    "\n",
    "    # Find the index of the maximum value\n",
    "    max_pos = np.unravel_index(np.argmax(i.T), i.shape)\n",
    "    optimal_pos.append(max_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds, base_chances = base_chance_threshold_fixed_prop([0,0.19], 0.2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:26<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "## ParD3\n",
    "\n",
    "thresholds, base_chances = base_chance_threshold_fixed_prop([0,0.19], 0.2, 5)\n",
    "splits = [20,15,10,5,1]\n",
    "\n",
    "optimal_base_chances = [base_chances[i[1]] for i in optimal_pos]\n",
    "optimal_splits = [splits[i[0]] for i in optimal_pos]\n",
    "\n",
    "x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, strategy = empirical_strategy_selection(ParD3_decay_multi, ParD3_sweep_multi, decay_rates, optimal_base_chances, optimal_splits,N=3)\n",
    "\n",
    "run = test_strategy_empirical(ParD3, [0.0,strategy[0]], \n",
    "                        [1,strategy[1]], \n",
    "                        [0.8,thresholds[np.array(base_chances) == strategy[0]]],\n",
    "                        starts=uniform_start_locs(ld=ParD3, num=10))\n",
    "\n",
    "## Saving data\n",
    "with open('plot_data/ParD3_strategy_selection.pkl', 'wb') as f:\n",
    "    pickle.dump((x_vals, decay_mean, decay_rate, sweep, scipy_freq_matrix, run), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
